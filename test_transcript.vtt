WEBVTT

1
00:00:01.120 --> 00:00:10.000
Ashish: We need to finalize the "Why Self-Attention" section. We're arguing that it's superior to recurrent and convolutional layers for long-range dependencies.

2
00:00:10.500 --> 00:00:22.000
Noam: Right, because the maximum path length between any two positions is a constant O(1) in the Transformer, whereas it's O(n) for recurrent layers.

3
00:00:22.100 --> 00:00:35.000
Niki: And for convolutional layers, it's logarithmic or linear depending on the kernel, which makes it harder to learn distant signals.

4
00:00:35.500 --> 00:00:48.000
Ashish: Exactly. Niki, what about the complexity per layer? Self-attention is faster when the sequence length n is smaller than the dimension d.

5
00:00:48.200 --> 00:01:02.000
Niki: Correct. It’s $O(n^2 \cdot d)$. If we hit very long sequences, we might need to look at restricted neighborhoods of size r, which increases path length to O(n/r).

6
00:01:02.500 --> 00:01:15.000
Illia: Let's discuss the encoder-decoder structure. The encoder maps input $x$ to continuous representations $z$, then the decoder generates output $y$ auto-regressively.

7
00:01:15.200 --> 00:01:25.000
Jakob: I think we should emphasize that each encoder layer has two sub-layers: multi-head self-attention and a position-wise feed-forward network.

8
00:01:25.500 --> 00:01:38.000
Ashish: Plus the residual connections around each sub-layer followed by layer normalization. We need that $LayerNorm(x + Sublayer(x))$ formula in the doc.

9
00:01:38.200 --> 00:01:50.000
Noam: Don't forget the dimensionality. All sub-layers and embedding layers produce outputs of dimension 512. It keeps the residuals consistent.

10
00:01:50.500 --> 00:02:05.000
Niki: I've finished the English-to-German experiments. The big model hit 28.4 BLEU. Even the base model at 27.3 BLEU beats the previous SOTA ensembles.

11
00:02:05.200 --> 00:02:18.000
Illia: That’s incredible. And the English-to-French score? I saw we reached 41.8 BLEU. The training cost was only 3.5 days on 8 P100s.

12
00:02:18.500 --> 00:02:32.000
Jakob: We should also mention the constituency parsing results. It proves the Transformer generalizes well to structural tasks with limited data.

13
00:02:32.100 --> 00:02:45.000
Noam: Agreed. The 4-layer Transformer we trained on WSJ data got 91.3 F1, which is impressive for a model not specifically tuned for parsing.

14
00:02:45.500 --> 00:02:58.000
Ashish: Finally, we need to clarify the positional encoding. We chose the sinusoidal version because it may allow the model to extrapolate to longer sequences.

15
00:02:58.200 --> 00:03:10.000
Niki: Right, we tried learned embeddings too, but the results were nearly identical. The sinusoids are just more elegant for our hypothesis.