[
  {
    "chunk_id": 1,
    "enhanced_content": "Here is a highly searchable description of the content:\n\n**Core Technical Facts and Data Points:**\n\n* The Transformer model was varied in different ways to evaluate the importance of its components.\n* The model variations were tested on the English-to-German translation development set, newstest2013.\n* The base model used a beam search with no checkpoint averaging.\n* The model variations included:\n\t+ Varying the number of attention heads (1, 4, 16, 32) and attention key and value dimensions (512, 128, 32, 16) while keeping computation constant.\n\t+ Reducing the attention key size (dk) and observing a drop in model quality.\n\t+ Increasing model size and observing improved performance.\n\t+ Applying dropout and observing improved performance in avoiding over-fitting.\n\t+ Replacing sinusoidal positional encoding with learned positional embeddings and observing nearly identical results.\n* Key performance metrics:\n\t+ BLEU scores: 65 (base model), 58-80 (varied models)\n\t+ Perplexities: 4.92-6.11 (varied models)\n\n**Visual Patterns or Diagrams Observed:**\n\n* The table (Table 3) presents the results of the model variations, with rows representing different variations and columns representing different metrics.\n* The table shows a clear pattern of performance changes as the model architecture is varied.\n\n**Summary of Topics for Vector Retrieval:**\n\n* **Transformer Model Variations**: The content discusses the importance of different components of the Transformer model and presents results on varying the model architecture.\n* **Machine Translation**: The content is focused on English-to-German translation and presents results on the development set, newstest2013.\n* **Attention Mechanisms**: The content discusses the importance of attention mechanisms and presents results on varying the number of attention heads and attention key and value dimensions.\n* **Model Size and Complexity**: The content presents results on increasing model size and observing improved performance.\n* **Dropout and Over-Fitting**: The content presents results on applying dropout and observing improved performance in avoiding over-fitting.\n* **Positional Encoding**: The content presents results on replacing sinusoidal positional encoding with learned positional embeddings.\n\n**Key Terms:**\n\n* Transformer model\n* Machine translation\n* Attention mechanisms\n* Model size and complexity\n* Dropout\n* Positional encoding\n* BLEU scores\n* Perplexities\n\n**Vector Retrieval Keywords:**\n\n* \"Transformer model variations\"\n* \"Machine translation development set\"\n* \"Attention mechanisms\"\n* \"Model size and complexity\"\n* \"Dropout and over-fitting\"\n* \"Positional encoding\"\n* \"BLEU scores\"\n* \"Perplexities\"",
    "metadata": {
      "document_name": "attention-is-all-you-need.pdf",
      "page_number": 8,
      "chunk_index": 19,
      "positions": [
        {
          "type": "Title",
          "coordinates": {
            "points": [
              [
                297.9297180175781,
                1826.0877316666665
              ],
              [
                297.9297180175781,
                1853.7616205555555
              ],
              [
                567.5433349609375,
                1853.7616205555555
              ],
              [
                567.5433349609375,
                1826.0877316666665
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "NarrativeText",
          "coordinates": {
            "points": [
              [
                294.76483154296875,
                1884.2481155555554
              ],
              [
                294.76483154296875,
                1942.2247822222223
              ],
              [
                1408.01416015625,
                1942.2247822222223
              ],
              [
                1408.01416015625,
                1884.2481155555554
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "UncategorizedText",
          "coordinates": {
            "points": [
              [
                335.14722222222224,
                1976.4015599999998
              ],
              [
                335.14722222222224,
                2005.3798399999998
              ],
              [
                1259.8777511111116,
                2005.3798399999998
              ],
              [
                1259.8777511111116,
                1976.4015599999998
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "Footer",
          "coordinates": {
            "points": [
              [
                842.71044921875,
                2061.325893333333
              ],
              [
                842.71044921875,
                2088.999782222222
              ],
              [
                857.803466796875,
                2088.999782222222
              ],
              [
                857.803466796875,
                2061.325893333333
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "NarrativeText",
          "coordinates": {
            "points": [
              [
                295.60198974609375,
                197.0675599999999
              ],
              [
                295.60198974609375,
                315.64978222222226
              ],
              [
                1404.6217041015625,
                315.64978222222226
              ],
              [
                1404.6217041015625,
                197.0675599999999
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "Table",
          "coordinates": {
            "points": [
              [
                300.62860107421875,
                356.70068359375
              ],
              [
                300.62860107421875,
                1062.924560546875
              ],
              [
                1406.045654296875,
                1062.924560546875
              ],
              [
                1406.045654296875,
                356.70068359375
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "UncategorizedText",
          "coordinates": {
            "points": [
              [
                330.42777777777775,
                1035.8064488888888
              ],
              [
                330.42777777777775,
                1063.4803377777778
              ],
              [
                365.79500777777776,
                1063.4803377777778
              ],
              [
                365.79500777777776,
                1035.8064488888888
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "UncategorizedText",
          "coordinates": {
            "points": [
              [
                411.6166666666666,
                1035.8064488888888
              ],
              [
                411.6166666666666,
                1063.4803377777778
              ],
              [
                425.45361111111106,
                1063.4803377777778
              ],
              [
                425.45361111111106,
                1035.8064488888888
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "UncategorizedText",
          "coordinates": {
            "points": [
              [
                468.790921111111,
                1035.8064488888888
              ],
              [
                468.790921111111,
                1063.4803377777778
              ],
              [
                524.1386988888888,
                1063.4803377777778
              ],
              [
                524.1386988888888,
                1035.8064488888888
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "UncategorizedText",
          "coordinates": {
            "points": [
              [
                561.8028616666667,
                1035.8064488888888
              ],
              [
                561.8028616666667,
                1063.4803377777778
              ],
              [
                617.1506394444443,
                1063.4803377777778
              ],
              [
                617.1506394444443,
                1035.8064488888888
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "UncategorizedText",
          "coordinates": {
            "points": [
              [
                650.3593061111111,
                1035.8064488888888
              ],
              [
                650.3593061111111,
                1063.4803377777778
              ],
              [
                678.033195,
                1063.4803377777778
              ],
              [
                678.033195,
                1035.8064488888888
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "UncategorizedText",
          "coordinates": {
            "points": [
              [
                875.3203488888889,
                1035.8064488888888
              ],
              [
                875.3203488888889,
                1063.4803377777778
              ],
              [
                909.91271,
                1063.4803377777778
              ],
              [
                909.91271,
                1035.8064488888888
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "UncategorizedText",
          "coordinates": {
            "points": [
              [
                1025.5618916666667,
                1035.6127316666668
              ],
              [
                1025.5618916666667,
                1063.4803377777778
              ],
              [
                1173.32375,
                1063.4803377777778
              ],
              [
                1173.32375,
                1035.6127316666668
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "UncategorizedText",
          "coordinates": {
            "points": [
              [
                1223.081402222222,
                1035.6127316666668
              ],
              [
                1223.081402222222,
                1063.2866205555556
              ],
              [
                1271.5107077777775,
                1063.2866205555556
              ],
              [
                1271.5107077777775,
                1035.6127316666668
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "NarrativeText",
          "coordinates": {
            "points": [
              [
                297.39324951171875,
                1148.5981155555555
              ],
              [
                297.39324951171875,
                1206.574782222222
              ],
              [
                1406.0859375,
                1206.574782222222
              ],
              [
                1406.0859375,
                1148.5981155555555
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "NarrativeText",
          "coordinates": {
            "points": [
              [
                294.759765625,
                1224.4258933333333
              ],
              [
                294.759765625,
                1312.7053377777777
              ],
              [
                1414.1932373046875,
                1312.7053377777777
              ],
              [
                1414.1932373046875,
                1224.4258933333333
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "NarrativeText",
          "coordinates": {
            "points": [
              [
                300.0,
                1329.9476233333332
              ],
              [
                300.0,
                1509.7442266666667
              ],
              [
                1407.969482421875,
                1509.7442266666667
              ],
              [
                1407.969482421875,
                1329.9476233333332
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        }
      ],
      "original_content": {
        "raw_text": "6.2 Model Variations\n\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\n\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\n\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\n\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.",
        "tables_html": [
          "<table><thead><tr><th></th><th>N</th><th>dwss</th><th>dn</th><th>b</th><th>di</th><th>o</th><th>Pug</th><th>as</th><th>o</th><th>| Gos</th><th>RIS</th><th>PR</th></tr></thead><tbody><tr><td>base</td><td>| 6</td><td>512</td><td>2048</td><td>8</td><td>64</td><td>64</td><td>0.1</td><td>01</td><td>100K</td><td>| 492</td><td>258</td><td>65</td></tr><tr><td rowspan=\"4\">(A)</td><td></td><td></td><td></td><td>1</td><td>512</td><td>512</td><td></td><td></td><td></td><td>529</td><td>249</td><td></td></tr><tr><td></td><td></td><td></td><td>4</td><td>128</td><td>128</td><td></td><td></td><td></td><td>500</td><td>255</td><td></td></tr><tr><td></td><td></td><td></td><td>16</td><td>32</td><td>32</td><td></td><td></td><td></td><td>491</td><td>258</td><td></td></tr><tr><td></td><td></td><td></td><td>32</td><td>16</td><td>16</td><td></td><td></td><td></td><td>501</td><td>254</td><td></td></tr><tr><td rowspan=\"2\">®)</td><td></td><td></td><td></td><td></td><td>16</td><td></td><td></td><td></td><td></td><td>516</td><td>251</td><td>58</td></tr><tr><td></td><td></td><td></td><td></td><td>32</td><td></td><td></td><td></td><td></td><td>501</td><td>254</td><td>60</td></tr><tr><td rowspan=\"7\">©)</td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>6.11</td><td>237</td><td>36</td></tr><tr><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>519</td><td>253</td><td>50</td></tr><tr><td>8</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>488</td><td>255</td><td>80</td></tr><tr><td></td><td>256</td><td></td><td></td><td>32</td><td>32</td><td></td><td></td><td></td><td>575</td><td>245</td><td>28</td></tr><tr><td></td><td>1024</td><td></td><td></td><td>128</td><td>128</td><td></td><td></td><td></td><td>4.66</td><td>260</td><td>168</td></tr><tr><td></td><td></td><td>1024</td><td></td><td></td><td></td><td></td><td></td><td></td><td>512</td><td>254</td><td>53</td></tr><tr><td></td><td></td><td>4096</td><td></td><td></td><td></td><td></td><td></td><td></td><td>475</td><td>262</td><td>90</td></tr><tr><td rowspan=\"4\">D</td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.0</td><td></td><td></td><td>577</td><td>246</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>0.2</td><td></td><td></td><td>495</td><td>255</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.0</td><td></td><td>467</td><td>253</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.2</td><td></td><td>547</td><td>257</td><td></td></tr><tr><td>(E)</td><td></td><td></td><td>positional</td><td>embedding</td><td></td><td>instead of</td><td>sinusoids</td><td></td><td></td><td>4.92</td><td>25.7</td><td></td></tr><tr><td>big</td><td>| 6</td><td>1024</td><td>4096</td><td>16</td><td></td><td></td><td>03</td><td></td><td>300K</td><td>| 433</td><td>264</td><td>213</td></tr></tbody></table>"
        ],
        "images_base64": []
      },
      "has_tables": true,
      "has_images": false
    }
  },
  {
    "chunk_id": 2,
    "enhanced_content": "6 Results\n\n6.1 Machine Translation\n\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\n\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38].\n\nTable 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5.",
    "metadata": {
      "document_name": "attention-is-all-you-need.pdf",
      "page_number": 8,
      "chunk_index": 18,
      "positions": [
        {
          "type": "Title",
          "coordinates": {
            "points": [
              [
                297.5052185058594,
                1030.5706577777778
              ],
              [
                297.5052185058594,
                1063.7795466666666
              ],
              [
                456.0323791503906,
                1063.7795466666666
              ],
              [
                456.0323791503906,
                1030.5706577777778
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "Title",
          "coordinates": {
            "points": [
              [
                296.4135437011719,
                1102.871065
              ],
              [
                296.4135437011719,
                1130.5449538888888
              ],
              [
                610.598876953125,
                1130.5449538888888
              ],
              [
                610.598876953125,
                1102.871065
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "NarrativeText",
          "coordinates": {
            "points": [
              [
                300.0,
                1161.0286711111112
              ],
              [
                300.0,
                1340.2192266666666
              ],
              [
                1411.6092529296875,
                1340.2192266666666
              ],
              [
                1411.6092529296875,
                1161.0286711111112
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "NarrativeText",
          "coordinates": {
            "points": [
              [
                298.73382568359375,
                1357.4587344444442
              ],
              [
                298.73382568359375,
                1478.5831033333332
              ],
              [
                1412.306640625,
                1478.5831033333332
              ],
              [
                1412.306640625,
                1357.4587344444442
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "NarrativeText",
          "coordinates": {
            "points": [
              [
                296.6873474121094,
                1494.5008933333334
              ],
              [
                296.6873474121094,
                1643.3858933333333
              ],
              [
                1412.0120849609375,
                1643.3858933333333
              ],
              [
                1412.0120849609375,
                1494.5008933333334
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "NarrativeText",
          "coordinates": {
            "points": [
              [
                295.337158203125,
                1661.2370044444442
              ],
              [
                295.337158203125,
                1779.8192266666665
              ],
              [
                1411.2901611328125,
                1779.8192266666665
              ],
              [
                1411.2901611328125,
                1661.2370044444442
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        }
      ],
      "original_content": {
        "raw_text": "6 Results\n\n6.1 Machine Translation\n\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\n\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38].\n\nTable 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5.",
        "tables_html": [],
        "images_base64": []
      },
      "has_tables": false,
      "has_images": false
    }
  },
  {
    "chunk_id": 3,
    "enhanced_content": "**Highly Searchable Description:**\n\n**Regularization Techniques in Machine Translation Models**\n\nThe content discusses the use of regularization techniques in training machine translation models, specifically in the context of the Transformer model. Two types of regularization are employed:\n\n1. **Residual Dropout**: Dropout is applied to the output of each sub-layer, before it is added to the sub-layer input and normalized. Additionally, dropout is applied to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. The dropout rate used is Pdrop = 0.1.\n2. **Label Smoothing**: Label smoothing of value ϵls = 0.1 is employed during training, which improves accuracy and BLEU score but hurts perplexity.\n\n**Model Performance Comparison**\n\nA table compares the performance of various machine translation models, including the Transformer model, on English-German (EN-DE) and English-French (EN-FR) translation tasks. The table highlights the following key data points:\n\n* **BLEU Scores**: The Transformer (big) model achieves a BLEU score of 41.8 on the EN-FR task, outperforming other models such as GNMT + RL Ensemble (41.16) and ConvS2S Ensemble (41.29).\n* **Training Costs**: The Transformer (base) model has a training cost of 3.3 × 10^8 FLOPs, while the Transformer (big) model has a training cost of 2.3 × 10^9 FLOPs.\n* **Model Variants**: The table compares the performance of various model variants, including ensemble models and models with different architectures (e.g., ByteNet, Deep-Att + PosUnk, GNMT + RL).\n\n**Visual Patterns and Diagrams**\n\nNo explicit diagrams or visual patterns are observed in the content. However, the table provides a clear comparison of the performance of various machine translation models.\n\n**Summary of Topics for Vector Retrieval**\n\nThe following topics can be used for vector retrieval:\n\n1. **Regularization techniques**: Dropout, label smoothing, residual dropout.\n2. **Machine translation models**: Transformer, GNMT + RL, ConvS2S, MoE, Deep-Att + PosUnk, ByteNet.\n3. **Model performance comparison**: BLEU scores, training costs, model variants.\n4. **Natural Language Processing (NLP)**: Machine translation, language modeling, sequence-to-sequence models.\n\n**Core Technical Facts and Data Points**\n\n1. **Dropout rate**: Pdrop = 0.1.\n2. **Label smoothing value**: ϵls = 0.1.\n3. **BLEU scores**: Transformer (big) achieves 41.8 on EN-FR task.\n4. **Training costs**: Transformer (base) has a training cost of 3.3 × 10^8 FLOPs.\n5. **Model variants**: Ensemble models, models with different architectures.",
    "metadata": {
      "document_name": "attention-is-all-you-need.pdf",
      "page_number": 7,
      "chunk_index": 17,
      "positions": [
        {
          "type": "Title",
          "coordinates": {
            "points": [
              [
                295.7310791015625,
                1920.046065
              ],
              [
                295.7310791015625,
                1947.719953888889
              ],
              [
                539.7354736328125,
                1947.719953888889
              ],
              [
                539.7354736328125,
                1920.046065
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "NarrativeText",
          "coordinates": {
            "points": [
              [
                298.7,
                1978.303671111111
              ],
              [
                298.7,
                2005.97756
              ],
              [
                924.0975952148438,
                2005.97756
              ],
              [
                924.0975952148438,
                1978.303671111111
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "Footer",
          "coordinates": {
            "points": [
              [
                842.6747436523438,
                2061.325893333333
              ],
              [
                842.6747436523438,
                2088.999782222222
              ],
              [
                857.786376953125,
                2088.999782222222
              ],
              [
                857.786376953125,
                2061.325893333333
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "FigureCaption",
          "coordinates": {
            "points": [
              [
                299.14166666666665,
                197.0675599999999
              ],
              [
                299.14166666666665,
                255.04422666666673
              ],
              [
                1400.008966666666,
                255.04422666666673
              ],
              [
                1400.008966666666,
                197.0675599999999
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "Table",
          "coordinates": {
            "points": [
              [
                362.89111328125,
                266.6315002441406
              ],
              [
                362.89111328125,
                676.1827392578125
              ],
              [
                1332.961669921875,
                676.1827392578125
              ],
              [
                1332.961669921875,
                266.6315002441406
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "NarrativeText",
          "coordinates": {
            "points": [
              [
                297.37445068359375,
                758.2682872222222
              ],
              [
                297.37445068359375,
                878.97477
              ],
              [
                1408.59326171875,
                878.97477
              ],
              [
                1408.59326171875,
                758.2682872222222
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "NarrativeText",
          "coordinates": {
            "points": [
              [
                292.45172119140625,
                919.1615122222221
              ],
              [
                292.45172119140625,
                977.7470044444443
              ],
              [
                1413.3358154296875,
                977.7470044444443
              ],
              [
                1413.3358154296875,
                919.1615122222221
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        }
      ],
      "original_content": {
        "raw_text": "5.4 Regularization\n\nWe employ three types of regularization during training:\n\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\n\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.",
        "tables_html": [
          "<table><thead><tr><th>Model</th><th>EN-DE</th><th>BLEU EN-FR</th><th>Training EN-DE</th><th>Cost (FLOPs) EN-FR</th></tr></thead><tbody><tr><td colspan=\"5\">ByteNet [18] 2375</td></tr><tr><td>Deep-Att + PosUnk</td><td></td><td>39.2</td><td></td><td>1.0-10%</td></tr><tr><td>GNMT + RL [38]</td><td>24.6</td><td>39.92</td><td>2.3-10°</td><td>1.4-10%</td></tr><tr><td>ConvS2S [9]</td><td>25.16</td><td>40.46</td><td>9.6-10'®</td><td>1.5.-10%</td></tr><tr><td>MoE</td><td>26.03</td><td>40.56</td><td>2.0-10°</td><td>1.2.10%</td></tr><tr><td>Deep-Att + PosUnk Ensemble</td><td></td><td>40.4</td><td></td><td>8.0-10%°</td></tr><tr><td>GNMT + RL Ensemble [38]</td><td>2630</td><td>41.16</td><td>1.8-10%°</td><td>1.1-10*</td></tr><tr><td>ConvS2S Ensemble [9]</td><td>2636</td><td>41.29</td><td>7.7-10°</td><td>1.2-10%</td></tr><tr><td>Transformer (base model)</td><td>273</td><td>38.1</td><td></td><td>3.3.10'8</td></tr><tr><td>Transformer (big)</td><td>28.4</td><td>41.8</td><td></td><td>2.3-101</td></tr></tbody></table>"
        ],
        "images_base64": []
      },
      "has_tables": true,
      "has_images": false
    }
  }
]