{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b3e564a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in ./venv/lib/python3.13/site-packages (1.4.0)\n",
      "Requirement already satisfied: build>=1.0.3 in ./venv/lib/python3.13/site-packages (from chromadb) (1.4.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in ./venv/lib/python3.13/site-packages (from chromadb) (2.12.5)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in ./venv/lib/python3.13/site-packages (from chromadb) (1.4.3)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.40.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in ./venv/lib/python3.13/site-packages (from chromadb) (2.2.6)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in ./venv/lib/python3.13/site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./venv/lib/python3.13/site-packages (from chromadb) (4.15.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./venv/lib/python3.13/site-packages (from chromadb) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./venv/lib/python3.13/site-packages (from chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./venv/lib/python3.13/site-packages (from chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in ./venv/lib/python3.13/site-packages (from chromadb) (1.39.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./venv/lib/python3.13/site-packages (from chromadb) (0.22.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./venv/lib/python3.13/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./venv/lib/python3.13/site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./venv/lib/python3.13/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in ./venv/lib/python3.13/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./venv/lib/python3.13/site-packages (from chromadb) (1.76.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./venv/lib/python3.13/site-packages (from chromadb) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./venv/lib/python3.13/site-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in ./venv/lib/python3.13/site-packages (from chromadb) (34.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in ./venv/lib/python3.13/site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in ./venv/lib/python3.13/site-packages (from chromadb) (6.0.3)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./venv/lib/python3.13/site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./venv/lib/python3.13/site-packages (from chromadb) (3.11.5)\n",
      "Requirement already satisfied: httpx>=0.27.0 in ./venv/lib/python3.13/site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./venv/lib/python3.13/site-packages (from chromadb) (14.2.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in ./venv/lib/python3.13/site-packages (from chromadb) (4.26.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.7 in ./venv/lib/python3.13/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.13/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in ./venv/lib/python3.13/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./venv/lib/python3.13/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in ./venv/lib/python3.13/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.13/site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (2026.1.4)\n",
      "Requirement already satisfied: packaging>=24.0 in ./venv/lib/python3.13/site-packages (from build>=1.0.3->chromadb) (25.0)\n",
      "Requirement already satisfied: pyproject_hooks in ./venv/lib/python3.13/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.13/site-packages (from httpx>=0.27.0->chromadb) (4.12.1)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.13/site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./venv/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./venv/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in ./venv/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb) (0.30.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in ./venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (2.47.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: requests-oauthlib in ./venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in ./venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.13/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./venv/lib/python3.13/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./venv/lib/python3.13/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: coloredlogs in ./venv/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./venv/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb) (25.12.19)\n",
      "Requirement already satisfied: protobuf in ./venv/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb) (6.33.3)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in ./venv/lib/python3.13/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.1)\n",
      "Requirement already satisfied: zipp>=3.20 in ./venv/lib/python3.13/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in ./venv/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.1 in ./venv/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.39.1 in ./venv/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in ./venv/lib/python3.13/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.60b1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.13/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./venv/lib/python3.13/site-packages (from pydantic>=1.9->chromadb) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./venv/lib/python3.13/site-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.13/site-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.13/site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in ./venv/lib/python3.13/site-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.0)\n",
      "Requirement already satisfied: click>=8.0.0 in ./venv/lib/python3.13/site-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./venv/lib/python3.13/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in ./venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in ./venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in ./venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./venv/lib/python3.13/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./venv/lib/python3.13/site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.13/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-google-genai in ./venv/lib/python3.13/site-packages (4.1.3)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in ./venv/lib/python3.13/site-packages (from langchain-google-genai) (1.2.0)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.56.0 in ./venv/lib/python3.13/site-packages (from langchain-google-genai) (1.57.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.5 in ./venv/lib/python3.13/site-packages (from langchain-google-genai) (1.2.7)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in ./venv/lib/python3.13/site-packages (from langchain-google-genai) (2.12.5)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in ./venv/lib/python3.13/site-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.12.1)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.46.0 in ./venv/lib/python3.13/site-packages (from google-auth[requests]<3.0.0,>=2.46.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.47.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in ./venv/lib/python3.13/site-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.28.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in ./venv/lib/python3.13/site-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.32.5)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in ./venv/lib/python3.13/site-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in ./venv/lib/python3.13/site-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in ./venv/lib/python3.13/site-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.15.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.13/site-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.9.0)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.13/site-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./venv/lib/python3.13/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (3.11)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.13/site-packages (from google-auth<3.0.0,>=2.46.0->google-auth[requests]<3.0.0,>=2.46.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./venv/lib/python3.13/site-packages (from google-auth<3.0.0,>=2.46.0->google-auth[requests]<3.0.0,>=2.46.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.13/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.13/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.16.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in ./venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in ./venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (0.6.2)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in ./venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in ./venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (6.0.3)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in ./venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (0.13.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./venv/lib/python3.13/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in ./venv/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./venv/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./venv/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./venv/lib/python3.13/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.46.0->google-auth[requests]<3.0.0,>=2.46.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-chroma in ./venv/lib/python3.13/site-packages (1.1.0)\n",
      "Requirement already satisfied: chromadb<2.0.0,>=1.3.5 in ./venv/lib/python3.13/site-packages (from langchain-chroma) (1.4.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.3 in ./venv/lib/python3.13/site-packages (from langchain-chroma) (1.2.7)\n",
      "Requirement already satisfied: numpy>=2.1.0 in ./venv/lib/python3.13/site-packages (from langchain-chroma) (2.2.6)\n",
      "Requirement already satisfied: build>=1.0.3 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.4.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (2.12.5)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.4.3)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.40.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (4.15.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.39.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.22.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.76.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.21.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (34.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (6.0.3)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (3.11.5)\n",
      "Requirement already satisfied: httpx>=0.27.0 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (14.2.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in ./venv/lib/python3.13/site-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (4.26.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in ./venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.1.3->langchain-chroma) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in ./venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.1.3->langchain-chroma) (0.6.2)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in ./venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.1.3->langchain-chroma) (25.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in ./venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.1.3->langchain-chroma) (0.13.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./venv/lib/python3.13/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.3->langchain-chroma) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./venv/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.3->langchain-chroma) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in ./venv/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.3->langchain-chroma) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./venv/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.3->langchain-chroma) (0.25.0)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.13/site-packages (from httpx>=0.27.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (4.12.1)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.13/site-packages (from httpx>=0.27.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.13/site-packages (from httpx>=0.27.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.0.9)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.13/site-packages (from httpx>=0.27.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.16.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.13/site-packages (from posthog<6.0.0,>=2.4.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in ./venv/lib/python3.13/site-packages (from posthog<6.0.0,>=2.4.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./venv/lib/python3.13/site-packages (from posthog<6.0.0,>=2.4.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in ./venv/lib/python3.13/site-packages (from posthog<6.0.0,>=2.4.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.13/site-packages (from pydantic>=1.9->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./venv/lib/python3.13/site-packages (from pydantic>=1.9->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./venv/lib/python3.13/site-packages (from pydantic>=1.9->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.3->langchain-chroma) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.3->langchain-chroma) (2.3.0)\n",
      "Requirement already satisfied: pyproject_hooks in ./venv/lib/python3.13/site-packages (from build>=1.0.3->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.2.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./venv/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./venv/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in ./venv/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.30.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in ./venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2.47.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.9.0)\n",
      "Requirement already satisfied: requests-oauthlib in ./venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2.0.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in ./venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.10)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.13/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./venv/lib/python3.13/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./venv/lib/python3.13/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.6.1)\n",
      "Requirement already satisfied: coloredlogs in ./venv/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.3.5->langchain-chroma) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./venv/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.3.5->langchain-chroma) (25.12.19)\n",
      "Requirement already satisfied: protobuf in ./venv/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.3.5->langchain-chroma) (6.33.3)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in ./venv/lib/python3.13/site-packages (from opentelemetry-api>=1.2.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (8.7.1)\n",
      "Requirement already satisfied: zipp>=3.20 in ./venv/lib/python3.13/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in ./venv/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.72.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.1 in ./venv/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.39.1 in ./venv/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in ./venv/lib/python3.13/site-packages (from opentelemetry-sdk>=1.2.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.60b1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.13/site-packages (from rich>=10.11.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.13/site-packages (from rich>=10.11.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in ./venv/lib/python3.13/site-packages (from tokenizers>=0.13.2->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.36.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb<2.0.0,>=1.3.5->langchain-chroma) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.2.0)\n",
      "Requirement already satisfied: click>=8.0.0 in ./venv/lib/python3.13/site-packages (from typer>=0.9.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./venv/lib/python3.13/site-packages (from typer>=0.9.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in ./venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.7.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.2.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in ./venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.22.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in ./venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.3.5->langchain-chroma) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./venv/lib/python3.13/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb<2.0.0,>=1.3.5->langchain-chroma) (10.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./venv/lib/python3.13/site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.13/site-packages (from sympy->onnxruntime>=1.14.1->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-groq in ./venv/lib/python3.13/site-packages (1.1.1)\n",
      "Requirement already satisfied: groq<1.0.0,>=0.30.0 in ./venv/lib/python3.13/site-packages (from langchain-groq) (0.37.1)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in ./venv/lib/python3.13/site-packages (from langchain-groq) (1.2.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.13/site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.13/site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.13/site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./venv/lib/python3.13/site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (2.12.5)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.13/site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in ./venv/lib/python3.13/site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->groq<1.0.0,>=0.30.0->langchain-groq) (3.11)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.16.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in ./venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in ./venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (0.6.2)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in ./venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in ./venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (9.1.2)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in ./venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (0.13.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./venv/lib/python3.13/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain-groq) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in ./venv/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./venv/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in ./venv/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./venv/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain-groq) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (2.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in ./venv/lib/python3.13/site-packages (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install chromadb\n",
    "%pip install langchain-google-genai\n",
    "%pip install langchain-chroma\n",
    "%pip install langchain-groq\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00132af6",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31c24ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a967c5",
   "metadata": {},
   "source": [
    "## 2. Initialize ChromaDB Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cc44605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ChromaDB initialized at: ./unified_chroma_db\n",
      "üìä Available collections: ['project_attention_transformer_project']\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB with persistent storage\n",
    "DB_PATH = \"./unified_chroma_db\"\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=DB_PATH,\n",
    "    settings=Settings(\n",
    "        anonymized_telemetry=False,\n",
    "        allow_reset=True\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ ChromaDB initialized at: {DB_PATH}\")\n",
    "print(f\"üìä Available collections: {[col.name for col in chroma_client.list_collections()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8cb84d",
   "metadata": {},
   "source": [
    "## 3. Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "529149fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding model loaded: Google Gemini text-embedding-004\n",
      "üìê Task type: retrieval_query\n"
     ]
    }
   ],
   "source": [
    "# Initialize Google Gemini embedding model (same as ingestion pipelines)\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/text-embedding-004\",\n",
    "    task_type=\"retrieval_query\"  # Use retrieval_query for queries\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Embedding model loaded: Google Gemini text-embedding-004\")\n",
    "print(f\"üìê Task type: retrieval_query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed3b1c",
   "metadata": {},
   "source": [
    "## 4. Load Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45256b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded collection: project_attention_transformer_project\n",
      "üìä Total chunks: 3\n",
      "   üìÑ Document chunks: 0\n",
      "   üéôÔ∏è  Transcript chunks: 3\n"
     ]
    }
   ],
   "source": [
    "def load_unified_database(project_name: str = \"default_project\"):\n",
    "    \"\"\"\n",
    "    Load the unified ChromaDB collection for a project.\n",
    "    \n",
    "    Args:\n",
    "        project_name: Name of the project\n",
    "        \n",
    "    Returns:\n",
    "        Chroma vector store object\n",
    "    \"\"\"\n",
    "    collection_name = f\"project_{project_name.lower().replace(' ', '_')}\"\n",
    "    \n",
    "    try:\n",
    "        # Load existing collection using LangChain wrapper\n",
    "        vectorstore = Chroma(\n",
    "            client=chroma_client,\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=embedding_model\n",
    "        )\n",
    "        \n",
    "        collection = chroma_client.get_collection(name=collection_name)\n",
    "        total_count = collection.count()\n",
    "        \n",
    "        # Get stats\n",
    "        all_data = collection.get(include=['metadatas'])\n",
    "        metadatas = all_data['metadatas']\n",
    "        \n",
    "        transcript_count = sum(1 for m in metadatas if m.get('source_type') == 'meeting_transcript')\n",
    "        document_count = sum(1 for m in metadatas if m.get('source_type') == 'document')\n",
    "        \n",
    "        print(f\"‚úÖ Loaded collection: {collection_name}\")\n",
    "        print(f\"üìä Total chunks: {total_count}\")\n",
    "        print(f\"   üìÑ Document chunks: {document_count}\")\n",
    "        print(f\"   üéôÔ∏è  Transcript chunks: {transcript_count}\")\n",
    "        \n",
    "        return vectorstore\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading collection '{collection_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example: Load your project\n",
    "PROJECT_NAME = \"attention_transformer_project\"  # Update to your project name\n",
    "db = load_unified_database(PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fede536",
   "metadata": {},
   "source": [
    "## 5. Query and Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74e65a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded collection: project_attention_transformer_project\n",
      "üìä Total chunks: 3\n",
      "   üìÑ Document chunks: 0\n",
      "   üéôÔ∏è  Transcript chunks: 3\n",
      "\n",
      "üîç Query: Who Completed the english to german experiments\n",
      "üìä Retrieving top 1 chunks...\n",
      "‚úÖ Retrieved 1 chunks\n",
      "   1. üéôÔ∏è  Transcript: transformer_meeting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='transcript_transformer_meeting_1', metadata={'end_time': '00:02:32.100', 'project_name': 'attention_transformer_project', 'speakers_in_chunk': '[\"Illia\", \"Ashish\", \"Niki\", \"Noam\", \"Jakob\"]', 'source_type': 'meeting_transcript', 'chunk_index': 1, 'meeting_date': '2026-01-11', 'start_time': '00:01:02.500', 'turn_count': 8, 'meeting_name': 'transformer_meeting'}, page_content=\"Meeting: transformer_meeting\\n        Project: attention_transformer_project\\n        Date: 2026-01-11\\n        Time Range: 00:01:02.500 - 00:02:32.100\\n        Speakers: Illia, Ashish, Niki, Noam, Jakob\\n\\n        Transcript:\\n        Illia: Let's discuss the encoder-decoder structure. The encoder maps input $x$ to continuous representations $z$, then the decoder generates output $y$ auto-regressively.\\nJakob: I think we should emphasize that each encoder layer has two sub-layers: multi-head self-attention and a position-wise feed-forward network.\\nAshish: Plus the residual connections around each sub-layer followed by layer normalization. We need that $LayerNorm(x + Sublayer(x))$ formula in the doc.\\nNoam: Don't forget the dimensionality. All sub-layers and embedding layers produce outputs of dimension 512. It keeps the residuals consistent.\\nNiki: I've finished the English-to-German experiments. The big model hit 28.4 BLEU. Even the base model at 27.3 BLEU beats the previous SOTA ensembles.\\nIllia: That‚Äôs incredible. And the English-to-French score? I saw we reached 41.8 BLEU. The training cost was only 3.5 days on 8 P100s.\\nJakob: We should also mention the constituency parsing results. It proves the Transformer generalizes well to structural tasks with limited data.\\nNoam: Agreed. The 4-layer Transformer we trained on WSJ data got 91.3 F1, which is impressive for a model not specifically tuned for parsing.\")]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def query_unified_collection(\n",
    "    query: str,\n",
    "    project_name: str = \"default_project\",\n",
    "    k: int = 5,\n",
    "    filter_source_type: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Query the unified collection and retrieve relevant chunks.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        project_name: Name of the project\n",
    "        k: Number of results to retrieve\n",
    "        filter_source_type: Optional filter (\"document\" or \"meeting_transcript\")\n",
    "        \n",
    "    Returns:\n",
    "        List of retrieved chunks\n",
    "    \"\"\"\n",
    "    vectorstore = load_unified_database(project_name)\n",
    "    \n",
    "    if not vectorstore:\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\nüîç Query: {query}\")\n",
    "    print(f\"üìä Retrieving top {k} chunks...\")\n",
    "    \n",
    "    # Build filter if specified\n",
    "    search_kwargs = {\"k\": k}\n",
    "    if filter_source_type:\n",
    "        search_kwargs[\"filter\"] = {\"source_type\": filter_source_type}\n",
    "        print(f\"üîé Filtering by source_type: {filter_source_type}\")\n",
    "    \n",
    "    # Create retriever and query\n",
    "    retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\n",
    "    retrieved_chunks = retriever.invoke(query)\n",
    "    \n",
    "    print(f\"‚úÖ Retrieved {len(retrieved_chunks)} chunks\")\n",
    "    \n",
    "    # Display chunk sources\n",
    "    for i, chunk in enumerate(retrieved_chunks):\n",
    "        source_type = chunk.metadata.get('source_type', 'unknown')\n",
    "        if source_type == 'meeting_transcript':\n",
    "            meeting = chunk.metadata.get('meeting_name', 'Unknown')\n",
    "            print(f\"   {i+1}. üéôÔ∏è  Transcript: {meeting}\")\n",
    "        else:\n",
    "            doc = chunk.metadata.get('document_name', 'Unknown')\n",
    "            page = chunk.metadata.get('page_number', 'N/A')\n",
    "            print(f\"   {i+1}. üìÑ Document: {doc} (p.{page})\")\n",
    "    \n",
    "    return retrieved_chunks\n",
    "\n",
    "\n",
    "# Test query\n",
    "query = \"Who Completed the english to german experiments\"\n",
    "retrieved_chunks = query_unified_collection(query, project_name=PROJECT_NAME, k=1)\n",
    "retrieved_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dad60d",
   "metadata": {},
   "source": [
    "## 6. Enhanced Citation Functions\n",
    "\n",
    "Support citations for both documents and transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5be6f7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Citation functions loaded\n"
     ]
    }
   ],
   "source": [
    "def extract_citations_with_metadata(answer_text, chunks_metadata):\n",
    "    \"\"\"\n",
    "    Extract citations and map to metadata for both documents and transcripts.\n",
    "    \n",
    "    Args:\n",
    "        answer_text: The generated answer with [CITE:X] markers\n",
    "        chunks_metadata: Dictionary mapping chunk IDs to metadata\n",
    "        \n",
    "    Returns:\n",
    "        List of citation dictionaries with source-specific metadata\n",
    "    \"\"\"\n",
    "    citation_pattern = r'\\[CITE:([0-9,\\s]+)\\]'\n",
    "    cited_chunks = re.findall(citation_pattern, answer_text)\n",
    "    \n",
    "    citations = []\n",
    "    unique_chunks = set()\n",
    "    \n",
    "    for cite_group in cited_chunks:\n",
    "        chunk_ids = [int(x.strip()) for x in cite_group.split(',')]\n",
    "        unique_chunks.update(chunk_ids)\n",
    "    \n",
    "    for chunk_id in sorted(unique_chunks):\n",
    "        metadata = chunks_metadata.get(chunk_id)\n",
    "        if metadata:\n",
    "            source_type = metadata.get(\"source_type\", \"document\")\n",
    "            \n",
    "            if source_type == \"meeting_transcript\":\n",
    "                # Transcript citation\n",
    "                citations.append({\n",
    "                    \"chunk_id\": str(chunk_id),\n",
    "                    \"source_type\": \"transcript\",\n",
    "                    \"meeting_name\": metadata.get(\"meeting_name\"),\n",
    "                    \"meeting_date\": metadata.get(\"meeting_date\"),\n",
    "                    \"start_time\": metadata.get(\"start_time\"),\n",
    "                    \"end_time\": metadata.get(\"end_time\"),\n",
    "                    \"speakers\": metadata.get(\"speakers\", [])\n",
    "                })\n",
    "            else:\n",
    "                # Document citation\n",
    "                citations.append({\n",
    "                    \"chunk_id\": str(chunk_id),\n",
    "                    \"source_type\": \"document\",\n",
    "                    \"document\": metadata.get(\"document\"),\n",
    "                    \"page\": metadata.get(\"page\"),\n",
    "                    \"positions\": metadata.get(\"positions\", [])\n",
    "                })\n",
    "    \n",
    "    return citations\n",
    "\n",
    "\n",
    "def format_answer_with_citations(answer_text, chunks_metadata):\n",
    "    \"\"\"\n",
    "    Replace [CITE:X] with appropriate format for documents and transcripts.\n",
    "    \n",
    "    Documents: [doc_name(p.X)]\n",
    "    Transcripts: [meeting_name(timestamp)]\n",
    "    \"\"\"\n",
    "    def replace_citation(match):\n",
    "        cite_group = match.group(1)\n",
    "        chunk_ids = [int(x.strip()) for x in cite_group.split(',')]\n",
    "        \n",
    "        # Group by source type\n",
    "        doc_chunks = []\n",
    "        transcript_chunks = []\n",
    "        \n",
    "        for chunk_id in chunk_ids:\n",
    "            metadata = chunks_metadata.get(chunk_id)\n",
    "            if metadata:\n",
    "                source_type = metadata.get(\"source_type\", \"document\")\n",
    "                if source_type == \"meeting_transcript\":\n",
    "                    transcript_chunks.append(metadata)\n",
    "                else:\n",
    "                    doc_chunks.append(metadata)\n",
    "        \n",
    "        # Format citations\n",
    "        citations = []\n",
    "        \n",
    "        # Format document citations\n",
    "        if doc_chunks:\n",
    "            doc_name = doc_chunks[0].get(\"document\", \"\").replace(\".pdf\", \"\")\n",
    "            pages = list(set(m.get(\"page\") for m in doc_chunks if m.get(\"page\")))\n",
    "            pages_str = \", \".join([f\"p.{p}\" for p in sorted(pages) if p != \"N/A\"])\n",
    "            if doc_name and pages_str:\n",
    "                citations.append(f\"{doc_name}({pages_str})\")\n",
    "        \n",
    "        # Format transcript citations\n",
    "        for transcript_meta in transcript_chunks:\n",
    "            meeting_name = transcript_meta.get(\"meeting_name\", \"Meeting\")\n",
    "            start_time = transcript_meta.get(\"start_time\", \"\")\n",
    "            if meeting_name:\n",
    "                time_suffix = f\"@{start_time}\" if start_time else \"\"\n",
    "                citations.append(f\"{meeting_name}{time_suffix}\")\n",
    "        \n",
    "        if citations:\n",
    "            return f\"[{', '.join(citations)}]\"\n",
    "        \n",
    "        return match.group(0)\n",
    "    \n",
    "    citation_pattern = r'\\[CITE:([0-9,\\s]+)\\]'\n",
    "    formatted_answer = re.sub(citation_pattern, replace_citation, answer_text)\n",
    "    return formatted_answer\n",
    "\n",
    "\n",
    "print(\"‚úÖ Citation functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cf646f",
   "metadata": {},
   "source": [
    "## 7. Answer Generation with Unified Context\n",
    "\n",
    "Generate answers using both document and transcript chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8bdea39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Answer generation function loaded\n"
     ]
    }
   ],
   "source": [
    "def generate_unified_answer(chunks, query):\n",
    "    \"\"\"\n",
    "    Generate answer with citations supporting both documents and transcripts.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of retrieved chunks (mix of documents and transcripts)\n",
    "        query: The user query\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with answer, raw_answer, and chunks_metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        llm = ChatGroq(\n",
    "            model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\", \n",
    "            temperature=0,\n",
    "            max_tokens=4096\n",
    "        )\n",
    "        \n",
    "        context_parts = []\n",
    "        all_images = []\n",
    "        chunks_metadata = {}\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_id = i + 1\n",
    "            source_type = chunk.metadata.get(\"source_type\", \"document\")\n",
    "            \n",
    "            # Build chunk header\n",
    "            if source_type == \"meeting_transcript\":\n",
    "                meeting_name = chunk.metadata.get(\"meeting_name\", \"Meeting\")\n",
    "                meeting_date = chunk.metadata.get(\"meeting_date\", \"\")\n",
    "                start_time = chunk.metadata.get(\"start_time\", \"\")\n",
    "                speakers_json = chunk.metadata.get(\"speakers_in_chunk\", \"[]\")\n",
    "                speakers = json.loads(speakers_json) if isinstance(speakers_json, str) else speakers_json\n",
    "                \n",
    "                doc_header = f\"### [CHUNK {chunk_id}] - TRANSCRIPT ###\\n\"\n",
    "                doc_header += f\"Meeting: {meeting_name}\\n\"\n",
    "                doc_header += f\"Date: {meeting_date}\\n\"\n",
    "                doc_header += f\"Time: {start_time}\\n\"\n",
    "                doc_header += f\"Speakers: {', '.join(speakers)}\\n\\n\"\n",
    "                \n",
    "                # Store metadata for citation\n",
    "                chunks_metadata[chunk_id] = {\n",
    "                    \"source_type\": \"meeting_transcript\",\n",
    "                    \"meeting_name\": meeting_name,\n",
    "                    \"meeting_date\": meeting_date,\n",
    "                    \"start_time\": start_time,\n",
    "                    \"end_time\": chunk.metadata.get(\"end_time\", \"\"),\n",
    "                    \"speakers\": speakers\n",
    "                }\n",
    "                \n",
    "                # Get transcript content\n",
    "                doc_body = chunk.page_content\n",
    "                \n",
    "            else:\n",
    "                # Document chunk\n",
    "                doc_header = f\"### [CHUNK {chunk_id}] - DOCUMENT ###\\n\"\n",
    "                doc_body = \"\"\n",
    "                \n",
    "                positions_str = chunk.metadata.get(\"positions\", \"[]\")\n",
    "                positions = json.loads(positions_str) if isinstance(positions_str, str) else positions_str\n",
    "                \n",
    "                # Store metadata for citation\n",
    "                chunks_metadata[chunk_id] = {\n",
    "                    \"source_type\": \"document\",\n",
    "                    \"page\": chunk.metadata.get(\"page_number\", \"N/A\"),\n",
    "                    \"document\": chunk.metadata.get(\"document_name\", \"document.pdf\"),\n",
    "                    \"positions\": positions\n",
    "                }\n",
    "                \n",
    "                # Extract content\n",
    "                if \"original_content\" in chunk.metadata:\n",
    "                    orig_data = json.loads(chunk.metadata[\"original_content\"])\n",
    "                    text = orig_data.get(\"raw_text\", \"\")\n",
    "                    tables = orig_data.get(\"tables_html\", [])\n",
    "                    \n",
    "                    doc_body += f\"TEXT CONTENT:\\n{text}\\n\"\n",
    "                    if tables:\n",
    "                        doc_body += \"\\nTABULAR DATA:\\n\" + \"\\n\".join(tables) + \"\\n\"\n",
    "                    \n",
    "                    all_images.extend(orig_data.get(\"images_base64\", []))\n",
    "                else:\n",
    "                    doc_body += chunk.page_content\n",
    "            \n",
    "            context_parts.append(doc_header + doc_body)\n",
    "\n",
    "        final_context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        instruction_prompt = f\"\"\"You are a precise research assistant. Answer the user query using ONLY the provided context.\n",
    "\n",
    "The context includes both DOCUMENT chunks (from PDFs) and TRANSCRIPT chunks (from meeting recordings).\n",
    "\n",
    "CITATION RULES:\n",
    "1. Add [CITE:X] citations ONLY after complete sentences or paragraphs\n",
    "2. NEVER add citations inside tables, lists, or mid-sentence\n",
    "3. For tables: Add a single citation AFTER the entire table\n",
    "4. Example: \"The results are shown below.\\n\\n[table here]\\n\\n[CITE:3]\"\n",
    "5. For information from multiple chunks, use [CITE:X, Y, Z] format\n",
    "6. You can cite both documents and transcripts - they are equally valid sources\n",
    "7. If information is not in the context, say \"I don't have information about that\"\n",
    "\n",
    "USER QUERY: {query}\n",
    "\n",
    "RESEARCH CONTEXT:\n",
    "{final_context}\n",
    "\n",
    "ANSWER (with [CITE:X] citations AFTER sentences/tables):\"\"\"\n",
    "\n",
    "        message_content = [{\"type\": \"text\", \"text\": instruction_prompt}]\n",
    "        \n",
    "        # Add images from document chunks (transcripts don't have images)\n",
    "        for img_b64 in all_images[:5]:\n",
    "            message_content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_b64}\"}\n",
    "            })\n",
    "        \n",
    "        response = llm.invoke([HumanMessage(content=message_content)])\n",
    "        \n",
    "        formatted_answer = format_answer_with_citations(response.content, chunks_metadata)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": formatted_answer,\n",
    "            \"raw_answer\": response.content,\n",
    "            \"chunks_metadata\": chunks_metadata\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Generation failed: {e}\")\n",
    "        return {\n",
    "            \"answer\": \"Error generating response\",\n",
    "            \"raw_answer\": \"\",\n",
    "            \"chunks_metadata\": {}\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"‚úÖ Answer generation function loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afe1330",
   "metadata": {},
   "source": [
    "## 8. Run Complete Query Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04caa7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üöÄ UNIFIED QUERY PIPELINE\n",
      "================================================================================\n",
      "‚úÖ Loaded collection: project_attention_transformer_project\n",
      "üìä Total chunks: 3\n",
      "   üìÑ Document chunks: 0\n",
      "   üéôÔ∏è  Transcript chunks: 3\n",
      "\n",
      "üîç Query: Who Completed the english to german experiments\n",
      "üìä Retrieving top 1 chunks...\n",
      "‚úÖ Retrieved 1 chunks\n",
      "   1. üéôÔ∏è  Transcript: transformer_meeting\n",
      "\n",
      "ü§ñ Generating answer with Llama 4 Scout...\n",
      "\n",
      "================================================================================\n",
      "üìù ANSWER:\n",
      "================================================================================\n",
      "Niki completed the English-to-German experiments. [transformer_meeting@00:01:02.500]\n",
      "\n",
      "================================================================================\n",
      "üìö CITATIONS:\n",
      "================================================================================\n",
      "\n",
      "üéôÔ∏è  Chunk 1 - Transcript Citation:\n",
      "   Meeting: transformer_meeting\n",
      "   Date: 2026-01-11\n",
      "   Time: 00:01:02.500 - 00:02:32.100\n",
      "   Speakers: Illia, Ashish, Niki, Noam, Jakob\n",
      "\n",
      "================================================================================\n",
      "‚úÖ QUERY PIPELINE COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURE YOUR QUERY HERE ===\n",
    "\n",
    "# Your question\n",
    "QUERY = \"Who Completed the english to german experiments\"\n",
    "\n",
    "# Project name (should match your ingestion pipelines)\n",
    "PROJECT_NAME = \"attention_transformer_project\"\n",
    "\n",
    "# Number of chunks to retrieve\n",
    "K_RESULTS = 1\n",
    "\n",
    "# Optional: Filter by source type (None, \"document\", or \"meeting_transcript\")\n",
    "FILTER_SOURCE = None  # Set to \"document\" or \"meeting_transcript\" to filter\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üöÄ UNIFIED QUERY PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 1: Retrieve chunks\n",
    "retrieved_chunks = query_unified_collection(\n",
    "    query=QUERY,\n",
    "    project_name=PROJECT_NAME,\n",
    "    k=K_RESULTS,\n",
    "    filter_source_type=FILTER_SOURCE\n",
    ")\n",
    "\n",
    "if not retrieved_chunks:\n",
    "    print(\"‚ùå No chunks retrieved\")\n",
    "else:\n",
    "    # Step 2: Generate answer\n",
    "    print(f\"\\nü§ñ Generating answer with Llama 4 Scout...\")\n",
    "    result = generate_unified_answer(retrieved_chunks, QUERY)\n",
    "    \n",
    "    # Step 3: Extract citations\n",
    "    citations = extract_citations_with_metadata(result[\"raw_answer\"], result[\"chunks_metadata\"])\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìù ANSWER:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(result[\"answer\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìö CITATIONS:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for cite in citations:\n",
    "        if cite['source_type'] == 'document':\n",
    "            print(f\"\\nüìÑ Chunk {cite['chunk_id']} - Document Citation:\")\n",
    "            print(f\"   Document: {cite['document']}\")\n",
    "            print(f\"   Page: {cite['page']}\")\n",
    "            print(f\"   Positions: {len(cite['positions'])} elements\")\n",
    "        else:\n",
    "            print(f\"\\nüéôÔ∏è  Chunk {cite['chunk_id']} - Transcript Citation:\")\n",
    "            print(f\"   Meeting: {cite['meeting_name']}\")\n",
    "            print(f\"   Date: {cite['meeting_date']}\")\n",
    "            print(f\"   Time: {cite['start_time']} - {cite['end_time']}\")\n",
    "            print(f\"   Speakers: {', '.join(cite['speakers'])}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ QUERY PIPELINE COMPLETE!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac1aeee",
   "metadata": {},
   "source": [
    "## 9. Export Results to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "671e7620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Exported results to: unified_query_results.json\n"
     ]
    }
   ],
   "source": [
    "def export_query_results(chunks, answer_result, filename=\"unified_query_results.json\"):\n",
    "    \"\"\"\n",
    "    Export query results including chunks, answer, and citations to JSON.\n",
    "    \n",
    "    Args:\n",
    "        chunks: Retrieved chunks\n",
    "        answer_result: Result from generate_unified_answer()\n",
    "        filename: Output filename\n",
    "    \"\"\"\n",
    "    export_data = {\n",
    "        \"query\": QUERY,\n",
    "        \"timestamp\": str(pd.Timestamp.now()) if 'pd' in dir() else None,\n",
    "        \"answer\": answer_result[\"answer\"],\n",
    "        \"raw_answer\": answer_result[\"raw_answer\"],\n",
    "        \"chunks\": [],\n",
    "        \"citations\": []\n",
    "    }\n",
    "    \n",
    "    # Export chunks\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_id = i + 1\n",
    "        source_type = chunk.metadata.get(\"source_type\", \"document\")\n",
    "        \n",
    "        chunk_data = {\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"source_type\": source_type,\n",
    "            \"content\": chunk.page_content,\n",
    "            \"metadata\": {}\n",
    "        }\n",
    "        \n",
    "        if source_type == \"meeting_transcript\":\n",
    "            chunk_data[\"metadata\"] = {\n",
    "                \"meeting_name\": chunk.metadata.get(\"meeting_name\"),\n",
    "                \"meeting_date\": chunk.metadata.get(\"meeting_date\"),\n",
    "                \"start_time\": chunk.metadata.get(\"start_time\"),\n",
    "                \"end_time\": chunk.metadata.get(\"end_time\"),\n",
    "                \"speakers\": json.loads(chunk.metadata.get(\"speakers_in_chunk\", \"[]\"))\n",
    "            }\n",
    "        else:\n",
    "            chunk_data[\"metadata\"] = {\n",
    "                \"document_name\": chunk.metadata.get(\"document_name\"),\n",
    "                \"page_number\": chunk.metadata.get(\"page_number\"),\n",
    "                \"has_tables\": \"original_content\" in chunk.metadata and \"tables_html\" in json.loads(chunk.metadata[\"original_content\"]),\n",
    "                \"has_images\": \"original_content\" in chunk.metadata and \"images_base64\" in json.loads(chunk.metadata[\"original_content\"])\n",
    "            }\n",
    "        \n",
    "        export_data[\"chunks\"].append(chunk_data)\n",
    "    \n",
    "    # Export citations\n",
    "    citations = extract_citations_with_metadata(answer_result[\"raw_answer\"], answer_result[\"chunks_metadata\"])\n",
    "    export_data[\"citations\"] = citations\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"‚úÖ Exported results to: {filename}\")\n",
    "\n",
    "\n",
    "# Export results\n",
    "if retrieved_chunks and result:\n",
    "    export_query_results(retrieved_chunks, result, \"unified_query_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f7757c",
   "metadata": {},
   "source": [
    "## 10. Advanced Query Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe6f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Query only documents\n",
    "print(\"\\nüìÑ DOCUMENT-ONLY QUERY:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "doc_chunks = query_unified_collection(\n",
    "    query=\"transformer architecture details\",\n",
    "    project_name=PROJECT_NAME,\n",
    "    k=3,\n",
    "    filter_source_type=\"document\"\n",
    ")\n",
    "\n",
    "if doc_chunks:\n",
    "    doc_result = generate_unified_answer(doc_chunks, \"transformer architecture details\")\n",
    "    print(doc_result[\"answer\"])\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Example: Query only transcripts\n",
    "print(\"üéôÔ∏è  TRANSCRIPT-ONLY QUERY:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "transcript_chunks = query_unified_collection(\n",
    "    query=\"what was discussed in the meeting\",\n",
    "    project_name=PROJECT_NAME,\n",
    "    k=3,\n",
    "    filter_source_type=\"meeting_transcript\"\n",
    ")\n",
    "\n",
    "if transcript_chunks:\n",
    "    transcript_result = generate_unified_answer(transcript_chunks, \"what was discussed in the meeting\")\n",
    "    print(transcript_result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a044638b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This unified query pipeline provides:\n",
    "\n",
    "‚úÖ **Unified Querying**: Query both documents and transcripts together  \n",
    "‚úÖ **Source-Aware Citations**: Different citation formats for documents vs transcripts  \n",
    "‚úÖ **Document Citations**: `[doc_name(p.X)]` with page numbers and positions  \n",
    "‚úÖ **Transcript Citations**: `[meeting_name@timestamp]` with meeting metadata  \n",
    "‚úÖ **Mixed Results**: AI can combine information from both sources  \n",
    "‚úÖ **Filtering Options**: Query all sources or filter by type  \n",
    "‚úÖ **Vision Support**: Process images from document chunks  \n",
    "‚úÖ **Export Results**: Save answers and citations to JSON  \n",
    "\n",
    "**Citation Formats:**\n",
    "- **Documents**: `[attention-is-all-you-need(p.3, p.5)]`\n",
    "- **Transcripts**: `[Project Kickoff Meeting@00:15:30]`\n",
    "- **Mixed**: `[attention-is-all-you-need(p.3), Project Kickoff Meeting@00:15:30]`\n",
    "\n",
    "**Next Steps:**\n",
    "1. Update `PROJECT_NAME` to match your ingestion pipelines\n",
    "2. Modify `QUERY` to ask your questions\n",
    "3. Run the pipeline to get answers with mixed sources\n",
    "4. Export results for further analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
