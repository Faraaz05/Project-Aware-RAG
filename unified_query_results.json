{
  "query": "Who worked on the English-to-German experiments? what are the detailed results of the work",
  "timestamp": null,
  "answer": "Niki worked on the English-to-German experiments [transformer_meeting@00:01:50.500]. The detailed results of the work are as follows: The big Transformer model achieved a BLEU score of 28.4, outperforming the best previously reported models, including ensembles, by more than 2.0 BLEU. This establishes a new state-of-the-art BLEU score. The base model also performed well, beating the previous SOTA ensembles with a BLEU score of 27.3. The training cost for the big model was 3.5 days on 8 P100 GPUs [attention-is-all-you-need(p.8), transformer_meeting@00:01:50.500].\n\nThe experiments used the WMT 2014 English-German dataset, consisting of about 4.5 million sentence pairs, and byte-pair encoding with a shared source-target vocabulary of about 37,000 tokens [attention-is-all-you-need(p.7)]. The results are summarized in Table 2, which compares the translation quality and training costs to other model architectures from the literature [attention-is-all-you-need(p.8)]. \n\n[attention-is-all-you-need(p.8), transformer_meeting@00:01:50.500]",
  "raw_answer": "Niki worked on the English-to-German experiments [CITE:1]. The detailed results of the work are as follows: The big Transformer model achieved a BLEU score of 28.4, outperforming the best previously reported models, including ensembles, by more than 2.0 BLEU. This establishes a new state-of-the-art BLEU score. The base model also performed well, beating the previous SOTA ensembles with a BLEU score of 27.3. The training cost for the big model was 3.5 days on 8 P100 GPUs [CITE:1, 3].\n\nThe experiments used the WMT 2014 English-German dataset, consisting of about 4.5 million sentence pairs, and byte-pair encoding with a shared source-target vocabulary of about 37,000 tokens [CITE:2]. The results are summarized in Table 2, which compares the translation quality and training costs to other model architectures from the literature [CITE:3]. \n\n[CITE:1, 3]",
  "chunks": [
    {
      "chunk_id": 1,
      "source_type": "meeting_transcript",
      "content": "Meeting: transformer_meeting\n        Project: attention_transformer_project\n        Date: 2026-01-11\n        Time Range: 00:01:50.500 - 00:02:45.500\n        Speakers: Illia, Ashish, Niki, Noam, Jakob\n\n        Transcript:\n        Niki: I've finished the English-to-German experiments. The big model hit 28.4 BLEU. Even the base model at 27.3 BLEU beats the previous SOTA ensembles.\nIllia: That’s incredible. And the English-to-French score? I saw we reached 41.8 BLEU. The training cost was only 3.5 days on 8 P100s.\nJakob: We should also mention the constituency parsing results. It proves the Transformer generalizes well to structural tasks with limited data.\nNoam: Agreed. The 4-layer Transformer we trained on WSJ data got 91.3 F1, which is impressive for a model not specifically tuned for parsing.\nAshish: Finally, we need to clarify the positional encoding. We chose the sinusoidal version because it may allow the model to extrapolate to longer sequences.",
      "metadata": {
        "meeting_name": "transformer_meeting",
        "meeting_date": "2026-01-11",
        "start_time": "00:01:50.500",
        "end_time": "00:02:45.500",
        "speakers": [
          "Illia",
          "Ashish",
          "Niki",
          "Noam",
          "Jakob"
        ]
      }
    },
    {
      "chunk_id": 2,
      "source_type": "document",
      "content": "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\n\n5 Training\n\nThis section describes the training regime for our models.\n\n5.1 Training Data and Batching\n\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.",
      "metadata": {
        "document_name": "attention-is-all-you-need.pdf",
        "page_number": 7,
        "has_tables": true,
        "has_images": true
      }
    },
    {
      "chunk_id": 3,
      "source_type": "document",
      "content": "6 Results\n\n6.1 Machine Translation\n\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\n\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38].\n\nTable 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5.",
      "metadata": {
        "document_name": "attention-is-all-you-need.pdf",
        "page_number": 8,
        "has_tables": true,
        "has_images": true
      }
    }
  ],
  "citations": [
    {
      "chunk_id": "1",
      "source_type": "transcript",
      "meeting_name": "transformer_meeting",
      "meeting_date": "2026-01-11",
      "start_time": "00:01:50.500",
      "end_time": "00:02:45.500",
      "speakers": [
        "Illia",
        "Ashish",
        "Niki",
        "Noam",
        "Jakob"
      ]
    },
    {
      "chunk_id": "2",
      "source_type": "document",
      "document": "attention-is-all-you-need.pdf",
      "page": 7,
      "positions": [
        {
          "type": "NarrativeText",
          "coordinates": {
            "points": [
              [
                296.8670959472656,
                660.6620044444444
              ],
              [
                296.8670959472656,
                779.2442266666665
              ],
              [
                1412.6036376953125,
                779.2442266666665
              ],
              [
                1412.6036376953125,
                660.6620044444444
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "Title",
          "coordinates": {
            "points": [
              [
                297.72869873046875,
                832.3151022222221
              ],
              [
                297.72869873046875,
                865.523991111111
              ],
              [
                472.8522666666666,
                865.523991111111
              ],
              [
                472.8522666666666,
                832.3151022222221
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "NarrativeText",
          "coordinates": {
            "points": [
              [
                296.9465637207031,
                904.9564488888889
              ],
              [
                296.9465637207031,
                932.6303377777776
              ],
              [
                944.09619140625,
                932.6303377777776
              ],
              [
                944.09619140625,
                904.9564488888889
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "Title",
          "coordinates": {
            "points": [
              [
                297.255615234375,
                979.1488427777776
              ],
              [
                297.255615234375,
                1006.8227316666666
              ],
              [
                695.8419189453125,
                1006.8227316666666
              ],
              [
                695.8419189453125,
                979.1488427777776
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "NarrativeText",
          "coordinates": {
            "points": [
              [
                298.7,
                1037.406448888889
              ],
              [
                298.7,
                1246.8997822222223
              ],
              [
                1411.23828125,
                1246.8997822222223
              ],
              [
                1411.23828125,
                1037.406448888889
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        }
      ]
    },
    {
      "chunk_id": "3",
      "source_type": "document",
      "document": "attention-is-all-you-need.pdf",
      "page": 8,
      "positions": [
        {
          "type": "Title",
          "coordinates": {
            "points": [
              [
                297.5052185058594,
                1030.5706577777778
              ],
              [
                297.5052185058594,
                1063.7795466666666
              ],
              [
                456.0323791503906,
                1063.7795466666666
              ],
              [
                456.0323791503906,
                1030.5706577777778
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "Title",
          "coordinates": {
            "points": [
              [
                296.4135437011719,
                1102.871065
              ],
              [
                296.4135437011719,
                1130.5449538888888
              ],
              [
                610.598876953125,
                1130.5449538888888
              ],
              [
                610.598876953125,
                1102.871065
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "NarrativeText",
          "coordinates": {
            "points": [
              [
                300.0,
                1161.0286711111112
              ],
              [
                300.0,
                1340.2192266666666
              ],
              [
                1411.6092529296875,
                1340.2192266666666
              ],
              [
                1411.6092529296875,
                1161.0286711111112
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "NarrativeText",
          "coordinates": {
            "points": [
              [
                298.73382568359375,
                1357.4587344444442
              ],
              [
                298.73382568359375,
                1478.5831033333332
              ],
              [
                1412.306640625,
                1478.5831033333332
              ],
              [
                1412.306640625,
                1357.4587344444442
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "NarrativeText",
          "coordinates": {
            "points": [
              [
                296.6873474121094,
                1494.5008933333334
              ],
              [
                296.6873474121094,
                1643.3858933333333
              ],
              [
                1412.0120849609375,
                1643.3858933333333
              ],
              [
                1412.0120849609375,
                1494.5008933333334
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        },
        {
          "type": "NarrativeText",
          "coordinates": {
            "points": [
              [
                295.337158203125,
                1661.2370044444442
              ],
              [
                295.337158203125,
                1779.8192266666665
              ],
              [
                1411.2901611328125,
                1779.8192266666665
              ],
              [
                1411.2901611328125,
                1661.2370044444442
              ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
          }
        }
      ]
    }
  ]
}